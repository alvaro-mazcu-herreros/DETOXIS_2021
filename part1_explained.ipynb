{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of TOXicity in comments in Spanish (DETOXIS 2021)\n",
    "\n",
    "## SESIÓN 2.1: Preprocesamiento y extracción de características\n",
    "\n",
    "### Realizado por Álvaro Mazcuñán y Miquel Marín"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lee el CSV con los datos de `DETOXIS`.\n",
    "\n",
    "También se crean dos funciones claves. La primera, `read_data`, obtiene a partir de los datos del CSV, las variables que necesitaremos para el análisis.\n",
    "\n",
    "La segunda de ellas, `tweet_preprocessing`, realiza el preprocesado correspondiente al primer apartado de esta tarea:\n",
    " - **Normalización**: Llevar todo el texto al mismo nivel\n",
    "     - Conversión de todas las letras a mayúsculas o minúsculas\n",
    "     - Tokenización\n",
    "     - Eliminación de ruido: caracteres no deseados, como URLs, signos de puntuación, caracteres especiales, etc\n",
    "     - Eliminación de palabras irrelevantes (stopwords)\n",
    " - **Stemming**: Obtener la raíz de una palabra\n",
    " - **Lematización**: Obtener la forma base (lema)\n",
    "\n",
    "Su output **NO** está tokenizado. Esto es así debido a que 3 de las 4 funciones que se emplean más tardes requieren que el corpus no esté tokenizado. Cuando se utilice `Word2Vec` ya se modificará la función para tener los mensajes tokenizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer tweets y preprocesado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/mique/Documents/CURSO 3 - SEMESTRE B/LENGUAJE NATURAL Y RECUPERACION DE LA INFORMACION/PRACTICAS/DATASET_DETOXIS.csv')\n",
    "\n",
    "def read_data(data):\n",
    "    comment = list(data['comment'])\n",
    "    toxicity = list(data['toxicity'])\n",
    "    toxicity_level = list(data['toxicity_level'])\n",
    "    return comment, toxicity, toxicity_level\n",
    "\n",
    "def tweet_preprocessing_not_tokenized(tweet):\n",
    "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
    "    tokenized_tweets = word_tokenize(tweet)\n",
    "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
    "    \n",
    "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
    "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
    "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
    "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
    "    return \" \".join(lemma_tweets) # NO TOKENIZADO\n",
    "    \n",
    "comments, toxicity, toxicity_level = read_data(data)\n",
    "tweets_cleaned = [tweet_preprocessing_not_tokenized(tweet) for tweet in comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista `tweets_cleaned` contiene cada tweet ya preprocesado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de palabras (Bag-of-Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera de las diferentes opciones que se van a implementar es la creación de una Bolsa de Palabras o `Bag-of-Words`. En este caso, el valor por defecto del parámetro `ngram_range` es (1,1), queriendo decir que obtiene la bolsa de palabras con unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(tweets_cleaned)\n",
    "\n",
    "X_bag_of_words = vectorizer.transform(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se muestra el contenido de `X_bag_of_words` debido a que es una matriz de 3463 filas por 12700 columnas donde cada entrada representa las veces que aparece una palabra del vocabulario en el texto.\n",
    "\n",
    "Si se desea conocer qué palabras se encuentran en el vocabulario, basta con ejecutar la siguiente línea: `vectorizer.get_feature_names()`\n",
    "\n",
    "Aquí se muestra como queda con solo 10 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aprendemo',\n",
       " 'aprenden',\n",
       " 'aprendida',\n",
       " 'aprendido',\n",
       " 'aprendieran',\n",
       " 'aprendái',\n",
       " 'aprendí',\n",
       " 'aprietan',\n",
       " 'aprobado',\n",
       " 'apropiación']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gramas de palabras. N-gramas de caracteres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se realizará la tarea por **N-gramas** debido a lo siguiente: Si creamos la lista de palabras mediante unigramas, entonces no se pueden capturar frases y expresiones de varias palabras, porque no tiene en cuenta el orden de las palabras.\n",
    "\n",
    "No obstante, si se hace uso de n-gramas, por ejemplo, bigramas o trigramas, entonces se tendrán en cuenta las apariciones de pares o tripletas de palabras consecutivas, pudiendo realizar análisis más interesantes.\n",
    "\n",
    "\n",
    "En este caso se realizará una bolsa de palabras utilizando **bigramas y trigramas**, que son N-gramas de 2 y 3 palabras respectivamente (pasando como parámetro `ngram_range=(2,3)`, ya que pensamos que, al tener un texto con gran cantidad de tweets, utilizando este enfoque se obtendrán mejores resultados cuando se realice la tarea de **Evaluación (2.3)** donde se entrenarán modelos como Regresión Logística, Máquina de Soporte Vectorial (SVM), árboles de decisión, etc.\n",
    "\n",
    "Además, se ha investigado acerca del \"valor óptimo\" de ngramas y, en la mayoría de papers relacionados con NLP, se comenta que bigramas y trigramas son los que producen mejores resultados: \n",
    "\n",
    "<blockquote> <p>Payal B. Awachate, Prof. Vivek P. Kshirsagar <em>Improved Twitter Sentiment Analysis Using N\n",
    "Gram Feature Selection and Combinations</em>. Computer Science and Engineering Department, Government College of Engineering, Aurangabad, India (September 2016)</p>\n",
    "</blockquote>\n",
    "\n",
    "Sin embargo, una vez nos encontremos en esa fase correspondiente y no se obtiene un buen accuracy, se podrá volver hacia atrás y cambiar algunos parámetros referentes a los N-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 3))\n",
    "counts = ngram_vectorizer.fit_transform(tweets_cleaned)\n",
    "counts.toarray().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en una medida numérica que expresa cuán relevante es una palabra para un texto en un\n",
    "corpus.\n",
    "\n",
    "La idea con TF-IDF es reducir el peso de los términos proporcionalmente al número de textos en los\n",
    "que aparecen. De esta forma, el valor de un término aumenta proporcionalmente al número de veces\n",
    "que aparece en el texto, y es compensado por su frecuencia en el corpus.\n",
    "\n",
    "Para realizar dicha tarea, basta con implementar las siguientes líneas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea la importancia de las palabras, basta con ejecutar la siguiente línea: `tfidf_vectorizer.vocabulary_`\n",
    "\n",
    "Aquí se muestra para las 10 primeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pensó 9003\n",
      "zumo 12654\n",
      "restar 10533\n",
      "gusta 5821\n",
      "afeitado 489\n",
      "seco 10902\n",
      "gent 5666\n",
      "asi 1169\n",
      "maten 7609\n",
      "alta 714\n"
     ]
    }
   ],
   "source": [
    "iter_ = 0\n",
    "for word in tfidf_vectorizer.vocabulary_.keys():\n",
    "    if iter_ < 10:\n",
    "        print(word, tfidf_vectorizer.vocabulary_[word])        \n",
    "    else:\n",
    "        break\n",
    "    iter_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, se pasa a Words Embeddings, con `Word2Vec`.\n",
    "\n",
    "Este modelo se encuentra disponible mediante dos formas:\n",
    " - CBOW: Continuous Bag-of-Words\n",
    " - Skip-Gram\n",
    " \n",
    "\n",
    "<blockquote>\n",
    "    Skip-Gram is more\n",
    "efficient with small training data. Moreover, infrequent words are well presented. On the other hand, CBOW is faster and works well with frequent words. \n",
    "    \n",
    "    <p>Marwa Naili, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala <em>Comparative study of work embedding methods in topic segmentation</em>. </p>\n",
    "    \n",
    "    \n",
    "</blockquote>\n",
    "\n",
    "El modelo Word2Vec, de forma resumida, realiza lo siguiente: dado un vocabulario generado con las palabras del corpus, el objetivo es, para una palabra dada, que el modelo nos diga la probabilidad de que cada palabra del vocabulario sea vecina de la primera.\n",
    "\n",
    "<img src=\"word2vec.png\" width=\"800\"/>\n",
    "\n",
    "Según lo que se ha podido leer en este paper, se ha decidido realizar el modelo de Word2Vec mediante `Skip-Gram`, ya que la muestra de tweets que se están tratando en esta base de datos es de alrededor de los 3000, pudiendo considerarse un muestra pequeña.\n",
    "\n",
    "\n",
    "Se modifica la función de antes para tener tokenizadas las palabras de cada tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocessing_tokenized(tweet):\n",
    "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
    "    tokenized_tweets = word_tokenize(tweet)\n",
    "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
    "    \n",
    "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
    "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
    "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
    "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
    "    return lemma_tweets # TOKENIZADO\n",
    "    \n",
    "comments, toxicity, toxicity_level = read_data(data)\n",
    "tweets_cleaned = [tweet_preprocessing_tokenized(tweet) for tweet in comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se pasa a crear el modelo de Word2Vec, añadiendo un parámetro `sg` donde se indica que se utilizará el enfoque de Skip-Gram `(sg=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        tweets_cleaned,\n",
    "        vector_size=30,\n",
    "        min_count=5,\n",
    "        sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar un ejemplo del resultado obtenido utilizando dicho modelo, se probará **model** para que busque por las palabras de mayor similitud con **casa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('visado', 0.9922193884849548),\n",
       " ('dentro', 0.9919291734695435),\n",
       " ('eeuu', 0.9911899566650391),\n",
       " ('viviendo', 0.9910387992858887),\n",
       " ('venir', 0.990635335445404),\n",
       " ('vinieron', 0.9904506206512451),\n",
       " ('inversión', 0.9902233481407166),\n",
       " ('estereotipo', 0.9901642203330994),\n",
       " ('pasando', 0.9900962114334106),\n",
       " ('ciudad', 0.9898791909217834)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('casa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}