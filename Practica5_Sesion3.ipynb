{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BETO_VERSION.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdlSbWdaYaSp"
      },
      "source": [
        "# Detection of TOXicity in comments in Spanish (DETOXIS 2021)\n",
        "\n",
        "## SESIÓN 2.3: Clasificador basado en BERT y evaluación\n",
        "\n",
        "### Realizado por Álvaro Mazcuñán y Miquel Marín"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiR6zc9IAnBk",
        "outputId": "2d4d6957-f7ff-47cb-f67e-8dc6b648fbbe"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMTf65y-90vg"
      },
      "source": [
        "**IMPORTANTE**: VER LA JUSTIFICACIÓN QUE SE HA COMENTADO AL FINAL DEL NOTEBOOK CUANDO SE ENTRENA EL MODELO BETO PARA LA VARIABLE DE `TOXICITY_LEVEL`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10AfSr515rfz"
      },
      "source": [
        "### Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEzig8hPVuHO"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, AutoTokenizer, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import warnings"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK-PrZ_PMnzK"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DenQ9yWPPsB4",
        "outputId": "3f985640-2167-4946-8158-4063f80cf7bc"
      },
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM3dbwqgKA6v"
      },
      "source": [
        "### Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "3EyFY1BBOhmC",
        "outputId": "5d5d0de7-63b4-4fd4-a0ec-ae798fb1bdad"
      },
      "source": [
        "df = pd.read_csv(\"DATASET_DETOXIS.csv\")\n",
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>thread_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>comment_level</th>\n",
              "      <th>comment</th>\n",
              "      <th>argumentation</th>\n",
              "      <th>constructiveness</th>\n",
              "      <th>positive_stance</th>\n",
              "      <th>negative_stance</th>\n",
              "      <th>target_person</th>\n",
              "      <th>target_group</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>mockery</th>\n",
              "      <th>insult</th>\n",
              "      <th>improper_language</th>\n",
              "      <th>aggressiveness</th>\n",
              "      <th>intolerance</th>\n",
              "      <th>toxicity</th>\n",
              "      <th>toxicity_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_000</td>\n",
              "      <td>0_002</td>\n",
              "      <td>0_002</td>\n",
              "      <td>1</td>\n",
              "      <td>Pensó: Zumo para restar.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_001</td>\n",
              "      <td>0_003</td>\n",
              "      <td>0_003</td>\n",
              "      <td>1</td>\n",
              "      <td>Como les gusta el afeitado en seco a esta gente.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_002</td>\n",
              "      <td>0_004</td>\n",
              "      <td>0_004</td>\n",
              "      <td>1</td>\n",
              "      <td>asi me gusta, que se maten entre ellos y en al...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_003</td>\n",
              "      <td>0_005</td>\n",
              "      <td>0_005</td>\n",
              "      <td>1</td>\n",
              "      <td>Loss mas valientes, los que mejor cortan nuest...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_004</td>\n",
              "      <td>0_006</td>\n",
              "      <td>0_006</td>\n",
              "      <td>1</td>\n",
              "      <td>Costumbres...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3458</th>\n",
              "      <td>MI</td>\n",
              "      <td>20_134</td>\n",
              "      <td>20_164</td>\n",
              "      <td>20_164</td>\n",
              "      <td>1</td>\n",
              "      <td>Ya decía yo que veía menos moros</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3459</th>\n",
              "      <td>MI</td>\n",
              "      <td>20_006</td>\n",
              "      <td>20_165</td>\n",
              "      <td>20_008</td>\n",
              "      <td>2</td>\n",
              "      <td>+1. Como lo sabes...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3460</th>\n",
              "      <td>MI</td>\n",
              "      <td>20_135</td>\n",
              "      <td>20_166</td>\n",
              "      <td>20_166</td>\n",
              "      <td>1</td>\n",
              "      <td>Seguirán cobrando paguitas en Marruecos,expoli...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3461</th>\n",
              "      <td>MI</td>\n",
              "      <td>20_136</td>\n",
              "      <td>20_167</td>\n",
              "      <td>20_167</td>\n",
              "      <td>1</td>\n",
              "      <td>pobres, se arriesgan en pateras porque huyen d...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3462</th>\n",
              "      <td>MI</td>\n",
              "      <td>20_137</td>\n",
              "      <td>20_168</td>\n",
              "      <td>20_168</td>\n",
              "      <td>1</td>\n",
              "      <td>Yo me quiero escapar también, dan paguita al l...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3463 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     topic thread_id comment_id  ... intolerance  toxicity toxicity_level\n",
              "0       CR     0_000      0_002  ...           0         1              1\n",
              "1       CR     0_001      0_003  ...           0         1              1\n",
              "2       CR     0_002      0_004  ...           1         1              2\n",
              "3       CR     0_003      0_005  ...           0         1              1\n",
              "4       CR     0_004      0_006  ...           0         1              1\n",
              "...    ...       ...        ...  ...         ...       ...            ...\n",
              "3458    MI    20_134     20_164  ...           0         1              1\n",
              "3459    MI    20_006     20_165  ...           0         0              0\n",
              "3460    MI    20_135     20_166  ...           1         1              1\n",
              "3461    MI    20_136     20_167  ...           0         1              1\n",
              "3462    MI    20_137     20_168  ...           0         1              1\n",
              "\n",
              "[3463 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPReM1ZhO-XY"
      },
      "source": [
        "subset = df[[\"comment\", \"toxicity\",\"toxicity_level\"]]\n",
        "subset_df = df[[\"comment\", \"toxicity\"]]\n",
        "subset_df2 = df[[\"comment\", \"toxicity_level\"]]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Wvn4jbKE7C"
      },
      "source": [
        "### Leer tweets y preprocesado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYkhyUVLPKs_"
      },
      "source": [
        "def tweet_preprocessing_not_tokenized(tweet):\n",
        "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
        "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
        "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
        "    tokenized_tweets = word_tokenize(tweet)\n",
        "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
        "    \n",
        "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
        "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
        "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
        "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
        "    return \" \".join(lemma_tweets) # NO TOKENIZADO\n",
        "\n",
        "preprocessing = lambda x: tweet_preprocessing_not_tokenized(x)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzAd48wvPLwS",
        "outputId": "3e04e670-349c-45f9-bcc7-98a839898421"
      },
      "source": [
        "subset['comment'] = pd.DataFrame(subset[\"comment\"].apply(preprocessing))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyA7k32rOuQu"
      },
      "source": [
        "### Modelo con GridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XS2232gYAnB"
      },
      "source": [
        "Se ha decidido evaluar un modelo de SVM, realizando previamente el TF-IDF, tal y como se hizo en la anterior práctica, utilizando la técnica de GridSearch y, de esta forma, intentar obtener los parámetros óptimos para dicho algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MNip_OAQC_g"
      },
      "source": [
        "X = subset[\"comment\"]\n",
        "y = subset[\"toxicity\"]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTiKkBsGPN3Q"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjaT4AzoOspj"
      },
      "source": [
        "tuned_parameters =  [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3],\n",
        "                     'C': [0.001, 0.10, 0.1, 10]},\n",
        "                    {'kernel': ['sigmoid'], 'gamma': [1e-2, 1e-3],\n",
        "                     'C': [0.001, 0.10, 0.1, 10] },{'kernel': ['linear'], 'C': [0.001, 0.10, 0.1, 10]}]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxTulw_eLlqG",
        "outputId": "0c80ea8e-0d1f-441e-b8ab-54319f2edfb3"
      },
      "source": [
        "kf = KFold(n_splits=5)\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  \n",
        "  tfidf_vect = TfidfVectorizer()\n",
        "  tfidf_vect.fit(X)\n",
        "  train_X_Tfidf = tfidf_vect.transform(X_train)\n",
        "  test_X_Tfidf = tfidf_vect.transform(X_test)\n",
        "\n",
        "  clf = GridSearchCV(SVC(), tuned_parameters)\n",
        "  clf.fit(train_X_Tfidf, y_train)\n",
        "\n",
        "  svm_pred=clf.predict(test_X_Tfidf)\n",
        "  print(\"\\t\\tf1-score: {}\".format(f1_score(y_test, svm_pred)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\tf1-score: 0.0\n",
            "\t\tf1-score: 0.3887323943661972\n",
            "\t\tf1-score: 0.417910447761194\n",
            "\t\tf1-score: 0.0\n",
            "\t\tf1-score: 0.39572192513368987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FziGRGiuWO1L"
      },
      "source": [
        "Tal y como se puede observar, el f1-score que se obtiene en este caso, utilizando GridSearch, es muy inferior al que se obtuvo en la anterior entrega sin utilizar dicha técnica para obtener los parámetros óptimos.\n",
        "\n",
        "\n",
        "A continuación, una vez realizada la tarea con GridSearch, se va a pasar a realizar un modelo BERT, concretamente una variante, BETO, el cual evalúa los comentarios/tweets en castellano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzvFJam_5wgp"
      },
      "source": [
        "#### Parámetros iniciales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qiwIW_TA8sJ",
        "outputId": "840915b9-7091-4aca-8f2e-56dd36844a4f"
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "MAX_LEN = 200\n",
        "BATCH_SIZE = 16\n",
        "NCLASSES = 2\n",
        "NCLASSES_LEVELS = 4\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33lX57UP56vh"
      },
      "source": [
        "### Carga del modelo - BETO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hS5yrJ96GiW"
      },
      "source": [
        "En este caso, se va a cargar el modelo `bert-base-spanish-wwm-uncased` el cual se refiere a la variante de BERT, BETO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DYz9giQXpsg"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT-goykG6WIz"
      },
      "source": [
        "A continuación se observan cada uno de los tweets para ver cuál es la longitud máxima y, de esa forma, asignarlo a la hora de entrenar el modelo. Se puede ver como dicho valor se acerca alrededor de 200 (valor asignado previamente) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NySaOHRwYfn8",
        "outputId": "36340eb2-9e19-430e-cb24-8231e876f908"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in subset_df.comment:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "NbBjL1lzYjGm",
        "outputId": "25bc7c11-6886-462f-bbe4-054ce0c04d20"
      },
      "source": [
        "figure(figsize=(8, 8), dpi=80)\n",
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x640 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z15uJMe6xn9"
      },
      "source": [
        "### Creación de Clases necesarias para inicializar el modelo que se evaluará posteriormente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BrSSqzhYruE"
      },
      "source": [
        "class TOXICITY_DATASET(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, labels, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    label = self.labels[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      max_length=self.max_len,\n",
        "      add_special_tokens=True,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'label': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp2FZeovCGRm"
      },
      "source": [
        "def data_loader(df, tokenizer, max_len, batch_size):\n",
        "  dataset = TOXICITY_DATASET(\n",
        "    reviews=df.comment.to_numpy(),\n",
        "    labels=df.toxicity.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4\n",
        "  )\n",
        "\n",
        "def data_loader_levels(df, tokenizer, max_len, batch_size):\n",
        "  dataset = TOXICITY_DATASET(\n",
        "    reviews=df.comment.to_numpy(),\n",
        "    labels=df.toxicity_level.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XTDWerv63X4"
      },
      "source": [
        "### División de los tweets en un conjunto de entrenamiento y de test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPw1I2w27AHq"
      },
      "source": [
        "#### División para la variable `Toxicity`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SuiX6eEeCpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3559ae-6937-4a5b-ec30-2a133fdd94f8"
      },
      "source": [
        "df_train, df_test = train_test_split(subset_df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "\n",
        "train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWxVBcJG7Ezx"
      },
      "source": [
        "#### División para la variable `Toxicity_level`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "armlbieH1mCe",
        "outputId": "9872064a-c914-49e3-d618-381e18f1cf6e"
      },
      "source": [
        "df_train_levels, df_test_levels = train_test_split(subset_df2, test_size=0.1, random_state=RANDOM_SEED, stratify=subset_df2[\"toxicity_level\"])\n",
        "\n",
        "train_data_loader_levels = data_loader_levels(df_train_levels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader_levels = data_loader_levels(df_test_levels, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olDRhlgK7c_C"
      },
      "source": [
        "### Clases con las funciones necesarias para entrenar y evaluar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nUKnLnoUu0Q"
      },
      "source": [
        "class BERTToxicityClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BERTToxicityClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, cls_output = self.bert(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            return_dict = False\n",
        "        )\n",
        "        drop_output = self.drop(cls_output)\n",
        "        output = self.linear(drop_output)\n",
        "        return output\n",
        "\n",
        "        \"\"\"\n",
        "        last_hidden_states = self.bert(input_ids=input_ids,\n",
        "                                       attention_mask=attention_mask, return_dict=False)\n",
        "        pooled_output = last_hidden_states[0][:,0,:]\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "        \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsBtAYfVEtWI",
        "outputId": "0284939e-8aa9-4dca-fdea-c170da890900"
      },
      "source": [
        "model = BERTToxicityClassifier(NCLASSES)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUfZ7QZ6FDGV"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSf8uFjnGLc1"
      },
      "source": [
        "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for batch in data_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"label\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions, correct_predictions.double() / n_examples, np.mean(losses), preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmQ4DfDIIZA3"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch[\"input_ids\"].to(device)\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      labels = batch[\"label\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions, correct_predictions.double() / n_examples, np.mean(losses), preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mnnPWmwQMgo"
      },
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vOosnJL7j5c"
      },
      "source": [
        "### Evaluación para `Toxicity`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t3jgQuj7tOn"
      },
      "source": [
        "Tal y como se puede observar en el output de abajo, el accuracy que se obtiene en este caso es de alrededor de 0.78, valor más que aceptable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_rreV4OJO3Y",
        "outputId": "9aa7a036-a74d-4ff2-bf33-f209a200aa87"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n",
        "  print('------------------')\n",
        "  train_pred, train_acc, train_loss = train_model(\n",
        "      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n",
        "  )\n",
        "  test_pred, test_acc, test_loss = eval_model(\n",
        "      model, test_data_loader, loss_fn, device, len(df_test)\n",
        "  )\n",
        "  print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n",
        "  print('Validación: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n",
        "  print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.08519597004812497, accuracy: 0.9794608472400512\n",
            "Validación: Loss: 0.988951710137454, accuracy: 0.7896253602305475\n",
            "\n",
            "Epoch 2 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.05188588005824922, accuracy: 0.9890885750962772\n",
            "Validación: Loss: 0.9899407073178075, accuracy: 0.7838616714697406\n",
            "\n",
            "Epoch 3 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.04527373280710517, accuracy: 0.9900513478818997\n",
            "Validación: Loss: 0.9899407073178075, accuracy: 0.7838616714697406\n",
            "\n",
            "Epoch 4 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.04710916069049675, accuracy: 0.9897304236200256\n",
            "Validación: Loss: 0.9899407073178075, accuracy: 0.7838616714697406\n",
            "\n",
            "Epoch 5 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.040420337934763385, accuracy: 0.9926187419768934\n",
            "Validación: Loss: 0.9899407073178075, accuracy: 0.7838616714697406\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub8W_wmf3mMm",
        "outputId": "5f848558-f9ab-4a04-a119-d8343ddfab82"
      },
      "source": [
        "model_levels = BERTToxicityClassifier(NCLASSES_LEVELS)\n",
        "model_levels = model_levels.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BV5hkL97qu4"
      },
      "source": [
        "### Evaluación para `Toxicity_level`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44gZqTKp8EfX"
      },
      "source": [
        "No obstante, para el caso de la variable `toxicity_level`, el accuracy es de 0.26, lo cual deja muchísimo que desear. Para la **última entrega** lo que se va a realizar es lo siguiente: debido a que dicha clase está muy desbalanceada, alrededor de 2000 muestras y, en este caso, se quiere predecir el nivel de toxicidad de un tweet según si el modelo previamente lo ha predecido como si ese tweet es tóxico o no.\n",
        "\n",
        "Por lo tanto, primero se va a hacer una predicción sobre la variable `toxicity` y, una vez obtenidas las predicciones, se va a obtener una nueva columna \"new_predictions\" y con ésta solamente se trabajará con aquellos tweets que el modelo haya predecido como tóxicos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bNdbO0w08Ff",
        "outputId": "55409994-91ba-46a6-bd05-e517e92c31f7"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n",
        "  print('------------------')\n",
        "  train_pred_levels, train_acc_levels, train_loss_levels, preds_ = train_model(\n",
        "      model_levels, train_data_loader_levels, loss_fn, optimizer, device, scheduler, len(df_train_levels)\n",
        "  )\n",
        "  test_pred_levels, test_acc_levels, test_loss_levels, preds_2 = eval_model(\n",
        "      model_levels, test_data_loader_levels, loss_fn, device, len(df_test_levels)\n",
        "  )\n",
        "  print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss_levels, train_acc_levels))\n",
        "  print('Validación: Loss: {}, accuracy: {}'.format(test_loss_levels, test_acc_levels))\n",
        "  print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 1.4385385372699835, accuracy: 0.17490372272143773\n",
            "Validación: Loss: 1.4060748490420254, accuracy: 0.26512968299711814\n",
            "\n",
            "Epoch 2 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 1.4377366927953867, accuracy: 0.18549422336328625\n",
            "Validación: Loss: 1.4060748490420254, accuracy: 0.26512968299711814\n",
            "\n",
            "Epoch 3 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 1.4329264243443807, accuracy: 0.1925545571245186\n",
            "Validación: Loss: 1.4060748490420254, accuracy: 0.26512968299711814\n",
            "\n",
            "Epoch 4 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 1.4294751320129786, accuracy: 0.20410783055198972\n",
            "Validación: Loss: 1.4060748490420254, accuracy: 0.26512968299711814\n",
            "\n",
            "Epoch 5 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 1.4301990527373094, accuracy: 0.1944801026957638\n",
            "Validación: Loss: 1.4060748490420254, accuracy: 0.26512968299711814\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}