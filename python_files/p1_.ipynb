{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of TOXicity in comments in Spanish (DETOXIS 2021)\n",
    "\n",
    "## SESIÓN 2.1: Preprocesamiento y extracción de características\n",
    "\n",
    "### Realizado por Álvaro Mazcuñán y Miquel Marín"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lee el CSV con los datos de `DETOXIS`.\n",
    "\n",
    "También se crean dos funciones claves. La primera, `read_data`, obtiene a partir de los datos del CSV, las variables que necesitaremos para el análisis.\n",
    "\n",
    "La segunda de ellas, `tweet_preprocessing`, realiza el preprocesado correspondiente al primer apartado de esta tarea:\n",
    " - **Normalización**: Llevar todo el texto al mismo nivel\n",
    "     - Conversión de todas las letras a mayúsculas o minúsculas\n",
    "     - Tokenización\n",
    "     - Eliminación de ruido: caracteres no deseados, como URLs, signos de puntuación, caracteres especiales, etc\n",
    "     - Eliminación de palabras irrelevantes (stopwords)\n",
    " - **Stemming**: Obtener la raíz de una palabra\n",
    " - **Lematización**: Obtener la forma base (lema)\n",
    "\n",
    "Su output **NO** está tokenizado. Esto es así debido a que 3 de las 4 funciones que se emplean más tardes requieren que el corpus no esté tokenizado. Cuando se utilice `Word2Vec` ya se modificará la función para tener los mensajes tokenizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer tweets y preprocesado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/DATASET_DETOXIS.csv')\n",
    "\n",
    "def read_data(data):\n",
    "    comment = list(data['comment'])\n",
    "    toxicity = list(data['toxicity'])\n",
    "    toxicity_level = list(data['toxicity_level'])\n",
    "    return comment, toxicity, toxicity_level\n",
    "\n",
    "def tweet_preprocessing_not_tokenized(tweet):\n",
    "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
    "    tokenized_tweets = word_tokenize(tweet)\n",
    "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
    "    \n",
    "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
    "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
    "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
    "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
    "    return \" \".join(lemma_tweets) # NO TOKENIZADO\n",
    "    \n",
    "comments, toxicity, toxicity_level = read_data(data)\n",
    "tweets_cleaned = [tweet_preprocessing_not_tokenized(tweet) for tweet in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pensó zumo restar'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tweets_cleaned[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista `tweets_cleaned` contiene cada tweet ya preprocesado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de palabras (Bag-of-Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera de las diferentes opciones que se van a implementar es la creación de una Bolsa de Palabras o `Bag-of-Words`. En este caso, el valor por defecto del parámetro `ngram_range` es (1,1), queriendo decir que obtiene la bolsa de palabras con unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(tweets_cleaned)\n",
    "\n",
    "X_bag_of_words = vectorizer.transform(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se muestra el contenido de `X_bag_of_words` debido a que es una matriz de 3463 filas por 12700 columnas donde cada entrada representa las veces que aparece una palabra del vocabulario en el texto.\n",
    "\n",
    "Si se desea conocer qué palabras se encuentran en el vocabulario, basta con ejecutar la siguiente línea: `vectorizer.get_feature_names()`\n",
    "\n",
    "Aquí se muestra como queda con solo 10 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['aprendemo',\n",
       " 'aprenden',\n",
       " 'aprendida',\n",
       " 'aprendido',\n",
       " 'aprendieran',\n",
       " 'aprendái',\n",
       " 'aprendí',\n",
       " 'aprietan',\n",
       " 'aprobado',\n",
       " 'apropiación']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gramas de palabras. N-gramas de caracteres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se realizará la tarea por **N-gramas** debido a lo siguiente: Si creamos la lista de palabras mediante unigramas, entonces no se pueden capturar frases y expresiones de varias palabras, porque no tiene en cuenta el orden de las palabras.\n",
    "\n",
    "No obstante, si se hace uso de n-gramas, por ejemplo, bigramas o trigramas, entonces se tendrán en cuenta las apariciones de pares o tripletas de palabras consecutivas, pudiendo realizar análisis más interesantes.\n",
    "\n",
    "\n",
    "En este caso se realizará una bolsa de palabras utilizando **bigramas y trigramas**, que son N-gramas de 2 y 3 palabras respectivamente (pasando como parámetro `ngram_range=(2,3)`, ya que pensamos que, al tener un texto con gran cantidad de tweets, utilizando este enfoque se obtendrán mejores resultados cuando se realice la tarea de **Evaluación (2.3)** donde se entrenarán modelos como Regresión Logística, Máquina de Soporte Vectorial (SVM), árboles de decisión, etc.\n",
    "\n",
    "Además, se ha investigado acerca del \"valor óptimo\" de ngramas y, en la mayoría de papers relacionados con NLP, se comenta que bigramas y trigramas son los que producen mejores resultados: \n",
    "\n",
    "<blockquote> <p>Payal B. Awachate, Prof. Vivek P. Kshirsagar <em>Improved Twitter Sentiment Analysis Using N\n",
    "Gram Feature Selection and Combinations</em>. Computer Science and Engineering Department, Government College of Engineering, Aurangabad, India (September 2016)</p>\n",
    "</blockquote>\n",
    "\n",
    "Sin embargo, una vez nos encontremos en esa fase correspondiente y no se obtiene un buen accuracy, se podrá volver hacia atrás y cambiar algunos parámetros referentes a los N-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 3))\n",
    "counts = ngram_vectorizer.fit_transform(tweets_cleaned)\n",
    "counts.toarray().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en una medida numérica que expresa cuán relevante es una palabra para un texto en un\n",
    "corpus.\n",
    "\n",
    "La idea con TF-IDF es reducir el peso de los términos proporcionalmente al número de textos en los\n",
    "que aparecen. De esta forma, el valor de un término aumenta proporcionalmente al número de veces\n",
    "que aparece en el texto, y es compensado por su frecuencia en el corpus.\n",
    "\n",
    "Para realizar dicha tarea, basta con implementar las siguientes líneas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea la importancia de las palabras, basta con ejecutar la siguiente línea: `tfidf_vectorizer.vocabulary_`\n",
    "\n",
    "Aquí se muestra para las 10 primeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pensó 9003\nzumo 12654\nrestar 10533\ngusta 5821\nafeitado 489\nseco 10902\ngent 5666\nasi 1169\nmaten 7609\nalta 714\n"
     ]
    }
   ],
   "source": [
    "iter_ = 0\n",
    "for word in tfidf_vectorizer.vocabulary_.keys():\n",
    "    if iter_ < 10:\n",
    "        print(word, tfidf_vectorizer.vocabulary_[word])        \n",
    "    else:\n",
    "        break\n",
    "    iter_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, se pasa a Words Embeddings, con `Word2Vec`.\n",
    "\n",
    "Este modelo se encuentra disponible mediante dos formas:\n",
    " - CBOW: Continuous Bag-of-Words\n",
    " - Skip-Gram\n",
    " \n",
    "\n",
    "<blockquote>\n",
    "    Skip-Gram is more\n",
    "efficient with small training data. Moreover, infrequent words are well presented. On the other hand, CBOW is faster and works well with frequent words. \n",
    "    \n",
    "    <p>Marwa Naili, Anja Habacha Chaibi, Henda Hajjami Ben Ghezala <em>Comparative study of work embedding methods in topic segmentation</em>. </p>\n",
    "    \n",
    "    \n",
    "</blockquote>\n",
    "\n",
    "El modelo Word2Vec, de forma resumida, realiza lo siguiente: dado un vocabulario generado con las palabras del corpus, el objetivo es, para una palabra dada, que el modelo nos diga la probabilidad de que cada palabra del vocabulario sea vecina de la primera.\n",
    "\n",
    "<img src=\"word2vec.png\" width=\"800\"/>\n",
    "\n",
    "Según lo que se ha podido leer en este paper, se ha decidido realizar el modelo de Word2Vec mediante `Skip-Gram`, ya que la muestra de tweets que se están tratando en esta base de datos es de alrededor de los 3000, pudiendo considerarse un muestra pequeña.\n",
    "\n",
    "\n",
    "Se modifica la función de antes para tener tokenizadas las palabras de cada tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocessing_tokenized(tweet):\n",
    "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
    "    tokenized_tweets = word_tokenize(tweet)\n",
    "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
    "    \n",
    "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
    "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
    "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
    "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
    "    return lemma_tweets # TOKENIZADO\n",
    "    \n",
    "comments, toxicity, toxicity_level = read_data(data)\n",
    "tweets_cleaned = [tweet_preprocessing_tokenized(tweet) for tweet in comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se pasa a crear el modelo de Word2Vec, añadiendo un parámetro `sg` donde se indica que se utilizará el enfoque de Skip-Gram `(sg=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        tweets_cleaned,\n",
    "        vector_size=30,\n",
    "        min_count=5,\n",
    "        sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar un ejemplo del resultado obtenido utilizando dicho modelo, se probará **model** para que busque por las palabras de mayor similitud con **casa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('dentro', 0.9911617040634155),\n",
       " ('venir', 0.9911611080169678),\n",
       " ('piso', 0.9903740286827087),\n",
       " ('visado', 0.9903460741043091),\n",
       " ('vinieron', 0.9898891448974609),\n",
       " ('2500', 0.989872395992279),\n",
       " ('derivar', 0.9898684620857239),\n",
       " ('viviendo', 0.98983234167099),\n",
       " ('pai', 0.9898146986961365),\n",
       " ('buscars', 0.989814043045044)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.wv.most_similar('casa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocessing_not_tokenized(tweet):\n",
    "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
    "    tokenized_tweets = word_tokenize(tweet)\n",
    "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
    "    \n",
    "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
    "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
    "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
    "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
    "    return \" \".join(lemma_tweets) # NO TOKENIZADO\n",
    "    \n",
    "comments, toxicity, toxicity_level = read_data(data)\n",
    "tweets_cleaned = [tweet_preprocessing_not_tokenized(tweet) for tweet in comments]\n",
    "\n",
    "model = Word2Vec(\n",
    "        tweets_cleaned,\n",
    "        vector_size=30,\n",
    "        min_count=5,\n",
    "        sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/DATASET_DETOXIS.csv')\n",
    "sample_data = data[[\"comment\", \"toxicity\",\"toxicity_level\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-19-8d11717da6f6>:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data['comment'] = pd.DataFrame(sample_data[\"comment\"].apply(preprocessing))\n"
     ]
    }
   ],
   "source": [
    "def tweet_preprocessing_not_tokenized(tweet):\n",
    "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
    "    tokenized_tweets = word_tokenize(tweet)\n",
    "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
    "    \n",
    "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
    "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
    "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
    "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
    "    return \" \".join(lemma_tweets) # NO TOKENIZADO\n",
    "\n",
    "preprocessing = lambda x: tweet_preprocessing_not_tokenized(x)\n",
    "\n",
    "\n",
    "sample_data['comment'] = pd.DataFrame(sample_data[\"comment\"].apply(preprocessing))\n",
    "train_X_, test_X_, train_Y_, test_Y_ = train_test_split(sample_data['comment'], sample_data['toxicity_level'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        tweets_cleaned,\n",
    "        vector_size=30,\n",
    "        min_count=1,\n",
    "        sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a9ce3be88da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of words:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    646\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "words = sorted(model.wv.vocab.key_to_index)\n",
    "print(\"Number of words:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(sample_data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "You must specify either total_examples or total_words, for proper learning-rate and progress calculations. If you've just built the vocabulary using the same corpus, using the count cached in the model is sufficient: total_examples=model.corpus_count.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-35b1ee0d54b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 \u001b[0;34m\"You must specify either total_examples or total_words, for proper learning-rate \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m                 \u001b[0;34m\"and progress calculations. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You must specify either total_examples or total_words, for proper learning-rate and progress calculations. If you've just built the vocabulary using the same corpus, using the count cached in the model is sufficient: total_examples=model.corpus_count."
     ]
    }
   ],
   "source": [
    "model.train(sample_data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ = TfidfVectorizer()\n",
    "tfidf_vect_.fit(sample_data['comment'])\n",
    "train_X_Tfidf_ = tfidf_vect_.transform(train_X_)\n",
    "test_X_Tfidf_ = tfidf_vect_.transform(test_X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(sample_data['comment'], sample_data['toxicity'], test_size=0.3)\n",
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "\n",
    "ngram_vectorizer.fit(sample_data['comment'])\n",
    "train_X_count_ = ngram_vectorizer.transform(train_X)\n",
    "test_X_count_ = ngram_vectorizer.transform(test_X)\n",
    "\n",
    "svm_clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svm_clf.fit(train_X_count_,train_Y)\n",
    "predictions_SVM = svm_clf.predict(test_X_count_)\n",
    "\n",
    "score_svm = f1_score(test_Y, predictions_SVM, average='macro')\n",
    "score_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "\n",
    "ngram_vectorizer.fit(sample_data['comment'])\n",
    "train_X_count_ = ngram_vectorizer.transform(train_X)\n",
    "test_X_count_ = ngram_vectorizer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svm_clf.fit(train_X_count_,train_Y)\n",
    "predictions_SVM = svm_clf.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6959421399589857"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "score_svm = f1_score(test_Y, predictions_SVM, average='macro')\n",
    "score_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}