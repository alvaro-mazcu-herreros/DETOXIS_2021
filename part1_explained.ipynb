{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## SESIÓN 2.1: Preprocesamiento y extracción de características.\n",
    "\n",
    "### Realizado por Álvaro Mazcuñán y Miquel Marín"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Se importan las librerías que se utilizarán a lo largo del ejercicio:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Se lee el CSV con los datos de DETOXIS.\n",
    "\n",
    "También se crean dos funciones claves. La primera, \"read_data\", obtiene a partir de los datos del CSV, las variables que necesitaremos para el análisis.\n",
    "\n",
    "La segunda de ellas, \"preprocess_tweet_text\", realiza el preprocesado correspondiente al primer apartdado de esta tarea. Su \"output\" NO está tokenizado. Esto es así debido a que 3 de las 4 funciones que se emplean más tardes requieren que el corpus no esté tokenizado. Cuando se utilice \"Word2Vec\" ya se modificará la función para tener los datos tokenizados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/mazcu/Documents/UPV/3º/2º Cuatrimestre/LNR/DATASET_DETOXIS.csv')\n",
    "\n",
    "def read_data(data):\n",
    "    text = list(data['comment'])\n",
    "    t1_label = list(data['toxicity'])\n",
    "    t2_label = list(data['toxicity_level'])\n",
    "    return text, t1_label, t2_label\n",
    "\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    # REMOVE URL\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE)\n",
    "    # REMOVE @ AND #\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet)\n",
    "    # REMOVE EMOJIS AND EMOTICONES\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet)\n",
    "    # REMOVE PUNCTUATIONS\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    #REMOVE STOPWORDS\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered = [w for w in tweet_tokens if not w in set(stopwords.words('spanish'))]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    return \" \".join(lemma_words) #NO TOKENIZADO\n",
    "    \n",
    "texts, t1_label, t2_label = read_data(data)\n",
    "tweets_cleaned = [preprocess_tweet_text(tweet) for tweet in texts]"
   ]
  },
  {
   "source": [
    "\"tweets_cleaned\" es una lista que contiene cada tweet ya preprocesado.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n",
    "Ahora, se pasa a la parte de extracción de características:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "La primera de las diferentes opciones que se van a implementar es la creación de una Bolsa de Palabras o \"Bag-of-Words\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(tweets_cleaned)\n",
    "\n",
    "X_bag_of_words = vectorizer.transform(tweets_cleaned)"
   ]
  },
  {
   "source": [
    "No se muestra el contenido de \"X_bag_of_words\" debido a que es una matriz de 3463 filas por 12700 columnas.\n",
    "\n",
    "Si se desea conocer qué palabras se encuentran en el vocabulario, basta con ejecutar la siguiente línea: vectorizer.get_feature_names()\n",
    "\n",
    "Aquí se muestra como queda con solo 10 palabras.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['aprendemo',\n",
       " 'aprenden',\n",
       " 'aprendida',\n",
       " 'aprendido',\n",
       " 'aprendieran',\n",
       " 'aprendái',\n",
       " 'aprendí',\n",
       " 'aprietan',\n",
       " 'aprobado',\n",
       " 'apropiación']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[1000:1010]"
   ]
  },
  {
   "source": [
    "A continuación, se realizará la tarea por trigramas, que son N-Gramas de 3 palabras."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "counts = ngram_vectorizer.fit_transform(tweets_cleaned)\n",
    "counts.toarray().astype(int)"
   ]
  },
  {
   "source": [
    "Para el TF-IDF, se implementa el siguiente código:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(tweets_cleaned)\n"
   ]
  },
  {
   "source": [
    "Si se desea la importancia de las palabras, basta con ejecutar la siguiente línea: tfidf_vectorizer.vocabulary_\n",
    "\n",
    "Aquí se muestra para las 10 primeras."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pensó 9003\nzumo 12654\nrestar 10533\ngusta 5821\nafeitado 489\nseco 10902\ngent 5666\nasi 1169\nmaten 7609\nalta 714\n"
     ]
    }
   ],
   "source": [
    "iter_ = 0\n",
    "for word in tfidf_vectorizer.vocabulary_.keys():\n",
    "    if iter_ < 10:\n",
    "        print(word, tfidf_vectorizer.vocabulary_[word])        \n",
    "    else:\n",
    "        break\n",
    "    iter_ += 1"
   ]
  },
  {
   "source": [
    "Por último, se pasa a \"Words Embeddings\", con \"Word2Vec\".\n",
    "\n",
    "Se modifica la función de antes para tener tokenizadas las paalabras de cada tweet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    # REMOVE URL\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE)\n",
    "    # REMOVE @ AND #\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet)\n",
    "    # REMOVE EMOJIS AND EMOTICONES\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet)\n",
    "    # REMOVE PUNCTUATIONS\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    #REMOVE STOPWORDS\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered = [w for w in tweet_tokens if not w in set(stopwords.words('spanish'))]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    return lemma_words #TOKENIZADO\n",
    "    \n",
    "texts, t1_label, t2_label = read_data(data)\n",
    "tweets_cleaned = [preprocess_tweet_text(tweet) for tweet in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        tweets_cleaned,\n",
    "        vector_size=30,\n",
    "        min_count=5)"
   ]
  },
  {
   "source": [
    "Se probará \"model\" para que busque por las palabras de mayor similitud con \"política\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('vox', 0.9992325305938721),\n",
       " ('part', 0.9991074800491333),\n",
       " ('idea', 0.9990335702896118),\n",
       " ('creo', 0.9990204572677612),\n",
       " ('trabajo', 0.9990178942680359),\n",
       " ('millon', 0.9989981651306152),\n",
       " ('demá', 0.9988898038864136),\n",
       " ('mismo', 0.9988840222358704),\n",
       " ('muerto', 0.9988511204719543),\n",
       " ('podemo', 0.9988441467285156)]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model.wv.most_similar('política')"
   ]
  }
 ]
}