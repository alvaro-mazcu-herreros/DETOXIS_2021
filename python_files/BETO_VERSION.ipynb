{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BETO_VERSION.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiR6zc9IAnBk",
        "outputId": "19b6bcb5-69b6-4afb-c6ab-0b1c24ee415e"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 11.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEzig8hPVuHO"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, AutoTokenizer, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qiwIW_TA8sJ",
        "outputId": "e365b650-c3b5-4072-958d-59e0c8347e10"
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "MAX_LEN = 200\n",
        "BATCH_SIZE = 16\n",
        "NCLASSES = 2\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDNntq6IWPX0"
      },
      "source": [
        "df = pd.read_csv(\"DATASET_DETOXIS.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdj2qvGlWzYO"
      },
      "source": [
        "subset_df = df[[\"comment\", \"toxicity\", \"toxicity_level\"]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "iHIK3cT3V4Lh",
        "outputId": "a279428a-345d-4198-d1d8-259bdeae4a68"
      },
      "source": [
        "subset_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>toxicity</th>\n",
              "      <th>toxicity_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pensó: Zumo para restar.</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Como les gusta el afeitado en seco a esta gente.</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asi me gusta, que se maten entre ellos y en al...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Loss mas valientes, los que mejor cortan nuest...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Costumbres...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3458</th>\n",
              "      <td>Ya decía yo que veía menos moros</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3459</th>\n",
              "      <td>+1. Como lo sabes...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3460</th>\n",
              "      <td>Seguirán cobrando paguitas en Marruecos,expoli...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3461</th>\n",
              "      <td>pobres, se arriesgan en pateras porque huyen d...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3462</th>\n",
              "      <td>Yo me quiero escapar también, dan paguita al l...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3463 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                comment  ...  toxicity_level\n",
              "0                              Pensó: Zumo para restar.  ...               1\n",
              "1      Como les gusta el afeitado en seco a esta gente.  ...               1\n",
              "2     asi me gusta, que se maten entre ellos y en al...  ...               2\n",
              "3     Loss mas valientes, los que mejor cortan nuest...  ...               1\n",
              "4                                         Costumbres...  ...               1\n",
              "...                                                 ...  ...             ...\n",
              "3458                   Ya decía yo que veía menos moros  ...               1\n",
              "3459                               +1. Como lo sabes...  ...               0\n",
              "3460  Seguirán cobrando paguitas en Marruecos,expoli...  ...               1\n",
              "3461  pobres, se arriesgan en pateras porque huyen d...  ...               1\n",
              "3462  Yo me quiero escapar también, dan paguita al l...  ...               1\n",
              "\n",
              "[3463 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DYz9giQXpsg"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NySaOHRwYfn8",
        "outputId": "df3fa770-3e24-41b9-d167-99770bf7d4ba"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in subset_df.comment:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "NbBjL1lzYjGm",
        "outputId": "722de07c-2207-4e84-d90d-fa4575e8a172"
      },
      "source": [
        "figure(figsize=(8, 8), dpi=80)\n",
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x640 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BrSSqzhYruE"
      },
      "source": [
        "class TOXICITY_DATASET(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, labels, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    label = self.labels[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      max_length=self.max_len,\n",
        "      add_special_tokens=True,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'label': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp2FZeovCGRm"
      },
      "source": [
        "def data_loader(df, tokenizer, max_len, batch_size):\n",
        "  dataset = TOXICITY_DATASET(\n",
        "    reviews=df.comment.to_numpy(),\n",
        "    labels=df.toxicity.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SuiX6eEeCpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cf1032-c6d3-47b9-98b1-f95cfeb58d93"
      },
      "source": [
        "df_train, df_test = train_test_split(subset_df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "\n",
        "train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nUKnLnoUu0Q"
      },
      "source": [
        "class BERTToxicityClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BERTToxicityClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, cls_output = self.bert(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            return_dict = False\n",
        "        )\n",
        "        drop_output = self.drop(cls_output)\n",
        "        output = self.linear(drop_output)\n",
        "        return output\n",
        "\n",
        "        \"\"\"\n",
        "        last_hidden_states = self.bert(input_ids=input_ids,\n",
        "                                       attention_mask=attention_mask, return_dict=False)\n",
        "        pooled_output = last_hidden_states[0][:,0,:]\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "        \"\"\""
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsBtAYfVEtWI",
        "outputId": "bde5717f-434b-4b6e-bb3b-9f230716455b"
      },
      "source": [
        "model = BERTToxicityClassifier(NCLASSES)\n",
        "model = model.to(device)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOLc8xUlFAHw",
        "outputId": "ab8282b7-4fdf-4a9e-a293-333331993bf2"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERTToxicityClassifier(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.3, inplace=False)\n",
            "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUfZ7QZ6FDGV"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSf8uFjnGLc1"
      },
      "source": [
        "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for batch in data_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"label\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmQ4DfDIIZA3"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch[\"input_ids\"].to(device)\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      labels = batch[\"label\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mnnPWmwQMgo"
      },
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_rreV4OJO3Y",
        "outputId": "67c2ad0d-bf3f-4844-fbff-13454ef794b4"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n",
        "  print('------------------')\n",
        "  train_acc, train_loss = train_model(\n",
        "      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n",
        "  )\n",
        "  test_acc, test_loss = eval_model(\n",
        "      model, test_data_loader, loss_fn, device, len(df_test)\n",
        "  )\n",
        "  print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n",
        "  print('Validación: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n",
        "  print('')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.5982996564645033, accuracy: 0.6870988446726572\n",
            "Validación: Loss: 0.496246223422614, accuracy: 0.7752161383285302\n",
            "\n",
            "Epoch 2 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.3877887680935554, accuracy: 0.8424261874197688\n",
            "Validación: Loss: 0.503839413550767, accuracy: 0.7694524495677233\n",
            "\n",
            "Epoch 3 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.20675247451290488, accuracy: 0.9338896020539152\n",
            "Validación: Loss: 0.7775186097080057, accuracy: 0.7752161383285302\n",
            "\n",
            "Epoch 4 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.11584855030434063, accuracy: 0.9695121951219512\n",
            "Validación: Loss: 0.9359299166297371, accuracy: 0.7838616714697406\n",
            "\n",
            "Epoch 5 de 5\n",
            "------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entrenamiento: Loss: 0.06793547698344367, accuracy: 0.9842747111681642\n",
            "Validación: Loss: 1.040887663733553, accuracy: 0.7925072046109509\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}