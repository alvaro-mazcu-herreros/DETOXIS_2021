{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Detection of TOXicity in comments in Spanish (DETOXIS 2021)\n",
    "\n",
    "## SESIÓN 2.2: Clasificación\n",
    "\n",
    "### Realizado por Álvaro Mazcuñán y Miquel Marín"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**ATENCIÓN** \n",
    "\n",
    "En este caso, para el word2vec se ha encontrado un corpus preentrenado de `Google News`. No obstante, debido a que pesa alrededor de 1.5 Gb, se ha decidido no entrenar modelos de Machine Learning con Word-Embeddings.\n",
    "\n",
    "Aún así, se adjunta a continuación el corpus del cual estamos hablando.\n",
    "\n",
    "https://github.com/mmihaltz/word2vec-GoogleNews-vectors \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Librerías"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Se importan las mismas librerías que se utilizaron en la parte anterior"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 103,
=======
   "execution_count": 1,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "source": [
    "#### Datos de DETOXIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A continuación se cargan los datos tal y como se hizo en la anterior entrega"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     topic thread_id comment_id reply_to  comment_level  \\\n",
       "0       CR     0_000      0_002    0_002              1   \n",
       "1       CR     0_001      0_003    0_003              1   \n",
       "2       CR     0_002      0_004    0_004              1   \n",
       "3       CR     0_003      0_005    0_005              1   \n",
       "4       CR     0_004      0_006    0_006              1   \n",
       "...    ...       ...        ...      ...            ...   \n",
       "3458    MI    20_134     20_164   20_164              1   \n",
       "3459    MI    20_006     20_165   20_008              2   \n",
       "3460    MI    20_135     20_166   20_166              1   \n",
       "3461    MI    20_136     20_167   20_167              1   \n",
       "3462    MI    20_137     20_168   20_168              1   \n",
       "\n",
       "                                                comment  argumentation  \\\n",
       "0                              Pensó: Zumo para restar.              0   \n",
       "1      Como les gusta el afeitado en seco a esta gente.              0   \n",
       "2     asi me gusta, que se maten entre ellos y en al...              0   \n",
       "3     Loss mas valientes, los que mejor cortan nuest...              0   \n",
       "4                                         Costumbres...              0   \n",
       "...                                                 ...            ...   \n",
       "3458                   Ya decía yo que veía menos moros              0   \n",
       "3459                               +1. Como lo sabes...              0   \n",
       "3460  Seguirán cobrando paguitas en Marruecos,expoli...              0   \n",
       "3461  pobres, se arriesgan en pateras porque huyen d...              0   \n",
       "3462  Yo me quiero escapar también, dan paguita al l...              0   \n",
       "\n",
       "      constructiveness  positive_stance  negative_stance  ...  target_group  \\\n",
       "0                    0                0                0  ...             0   \n",
       "1                    0                0                0  ...             1   \n",
       "2                    0                0                0  ...             1   \n",
       "3                    0                0                0  ...             1   \n",
       "4                    0                0                0  ...             1   \n",
       "...                ...              ...              ...  ...           ...   \n",
       "3458                 0                0                0  ...             1   \n",
       "3459                 0                1                0  ...             0   \n",
       "3460                 0                0                0  ...             1   \n",
       "3461                 0                0                0  ...             1   \n",
       "3462                 0                0                0  ...             0   \n",
       "\n",
       "      stereotype  sarcasm  mockery  insult  improper_language  aggressiveness  \\\n",
       "0              0        0        1       0                  0               0   \n",
       "1              1        1        1       0                  0               0   \n",
       "2              0        0        0       0                  0               1   \n",
       "3              0        1        1       0                  0               0   \n",
       "4              1        0        0       0                  0               0   \n",
       "...          ...      ...      ...     ...                ...             ...   \n",
       "3458           0        0        1       1                  0               0   \n",
       "3459           0        0        0       0                  0               0   \n",
       "3460           1        0        0       0                  0               0   \n",
       "3461           0        0        1       0                  0               0   \n",
       "3462           0        0        1       0                  0               0   \n",
       "\n",
       "      intolerance  toxicity  toxicity_level  \n",
       "0               0         1               1  \n",
       "1               0         1               1  \n",
       "2               1         1               2  \n",
       "3               0         1               1  \n",
       "4               0         1               1  \n",
       "...           ...       ...             ...  \n",
       "3458            0         1               1  \n",
       "3459            0         0               0  \n",
       "3460            1         1               1  \n",
       "3461            0         1               1  \n",
       "3462            0         1               1  \n",
       "\n",
       "[3463 rows x 21 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>thread_id</th>\n      <th>comment_id</th>\n      <th>reply_to</th>\n      <th>comment_level</th>\n      <th>comment</th>\n      <th>argumentation</th>\n      <th>constructiveness</th>\n      <th>positive_stance</th>\n      <th>negative_stance</th>\n      <th>...</th>\n      <th>target_group</th>\n      <th>stereotype</th>\n      <th>sarcasm</th>\n      <th>mockery</th>\n      <th>insult</th>\n      <th>improper_language</th>\n      <th>aggressiveness</th>\n      <th>intolerance</th>\n      <th>toxicity</th>\n      <th>toxicity_level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CR</td>\n      <td>0_000</td>\n      <td>0_002</td>\n      <td>0_002</td>\n      <td>1</td>\n      <td>Pensó: Zumo para restar.</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CR</td>\n      <td>0_001</td>\n      <td>0_003</td>\n      <td>0_003</td>\n      <td>1</td>\n      <td>Como les gusta el afeitado en seco a esta gente.</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CR</td>\n      <td>0_002</td>\n      <td>0_004</td>\n      <td>0_004</td>\n      <td>1</td>\n      <td>asi me gusta, que se maten entre ellos y en al...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CR</td>\n      <td>0_003</td>\n      <td>0_005</td>\n      <td>0_005</td>\n      <td>1</td>\n      <td>Loss mas valientes, los que mejor cortan nuest...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CR</td>\n      <td>0_004</td>\n      <td>0_006</td>\n      <td>0_006</td>\n      <td>1</td>\n      <td>Costumbres...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3458</th>\n      <td>MI</td>\n      <td>20_134</td>\n      <td>20_164</td>\n      <td>20_164</td>\n      <td>1</td>\n      <td>Ya decía yo que veía menos moros</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3459</th>\n      <td>MI</td>\n      <td>20_006</td>\n      <td>20_165</td>\n      <td>20_008</td>\n      <td>2</td>\n      <td>+1. Como lo sabes...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3460</th>\n      <td>MI</td>\n      <td>20_135</td>\n      <td>20_166</td>\n      <td>20_166</td>\n      <td>1</td>\n      <td>Seguirán cobrando paguitas en Marruecos,expoli...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3461</th>\n      <td>MI</td>\n      <td>20_136</td>\n      <td>20_167</td>\n      <td>20_167</td>\n      <td>1</td>\n      <td>pobres, se arriesgan en pateras porque huyen d...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3462</th>\n      <td>MI</td>\n      <td>20_137</td>\n      <td>20_168</td>\n      <td>20_168</td>\n      <td>1</td>\n      <td>Yo me quiero escapar también, dan paguita al l...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3463 rows × 21 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "data = pd.read_csv('data/DATASET_DETOXIS.csv')\n",
    "data"
   ]
  },
  {
   "source": [
    "#### Subset de variables para el análisis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                comment  toxicity  \\\n",
       "0                              Pensó: Zumo para restar.         1   \n",
       "1      Como les gusta el afeitado en seco a esta gente.         1   \n",
       "2     asi me gusta, que se maten entre ellos y en al...         1   \n",
       "3     Loss mas valientes, los que mejor cortan nuest...         1   \n",
       "4                                         Costumbres...         1   \n",
       "...                                                 ...       ...   \n",
       "3458                   Ya decía yo que veía menos moros         1   \n",
       "3459                               +1. Como lo sabes...         0   \n",
       "3460  Seguirán cobrando paguitas en Marruecos,expoli...         1   \n",
       "3461  pobres, se arriesgan en pateras porque huyen d...         1   \n",
       "3462  Yo me quiero escapar también, dan paguita al l...         1   \n",
       "\n",
       "      toxicity_level  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  2  \n",
       "3                  1  \n",
       "4                  1  \n",
       "...              ...  \n",
       "3458               1  \n",
       "3459               0  \n",
       "3460               1  \n",
       "3461               1  \n",
       "3462               1  \n",
       "\n",
       "[3463 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>toxicity</th>\n      <th>toxicity_level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pensó: Zumo para restar.</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Como les gusta el afeitado en seco a esta gente.</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>asi me gusta, que se maten entre ellos y en al...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Loss mas valientes, los que mejor cortan nuest...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Costumbres...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3458</th>\n      <td>Ya decía yo que veía menos moros</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3459</th>\n      <td>+1. Como lo sabes...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3460</th>\n      <td>Seguirán cobrando paguitas en Marruecos,expoli...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3461</th>\n      <td>pobres, se arriesgan en pateras porque huyen d...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3462</th>\n      <td>Yo me quiero escapar también, dan paguita al l...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3463 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "sample_data = data[[\"comment\", \"toxicity\",\"toxicity_level\"]]\n",
    "sample_data"
   ]
  },
  {
   "source": [
    "#### Leer tweets y preprocesado "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocessing_not_tokenized(tweet):\n",
    "    tweet = tweet.lower() # Se empieza pasando todos los mensajes a minúsculas\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\" ,tweet , flags=re.MULTILINE) # Quitar URLs\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\", tweet) # Quitar @ y #\n",
    "    tweet = re.sub(r\"[\\U00010000-\\U0010ffff]|:\\)|:\\(|XD|xD|;\\)|:,\\(|:D|D:\", \"\", tweet) # Quitar emojis y emoticones\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Quitar signos de puntuación\n",
    "    tokenized_tweets = word_tokenize(tweet)\n",
    "    filtered_tweets = [word for word in tokenized_tweets if not word in set(stopwords.words('spanish'))] # Quitar stopwords y filtrar\n",
    "    \n",
    "    stemming = PorterStemmer() # Inicializamos PorterStemmer para obtener la raíz de cada una de las palabras\n",
    "    stemmed_tweets = [stemming.stem(word) for word in filtered_tweets]\n",
    "    lemmatization = WordNetLemmatizer() # Inicializamos el Lemmatizer para obtener los lemas de las palabras\n",
    "    lemma_tweets = [lemmatization.lemmatize(word, pos='a') for word in stemmed_tweets] \n",
    "    return \" \".join(lemma_tweets) # NO TOKENIZADO\n",
    "\n",
    "preprocessing = lambda x: tweet_preprocessing_not_tokenized(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-107-3c4e5d0f187f>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data['comment'] = pd.DataFrame(sample_data[\"comment\"].apply(preprocessing))\n"
     ]
    }
   ],
   "source": [
    "sample_data['comment'] = pd.DataFrame(sample_data[\"comment\"].apply(preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pensó zumo restar'"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "sample_data[\"comment\"][0]"
   ]
  },
  {
   "source": [
    "#### Problema del desbalance de clases"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Debido a que se van a usar algoritmos de clasificación se tendría que estudiar, para los dos variables de `toxicity` y `toxicity_level`, si dichas clases están balanceadas o no debido a que en problemas de clasificación se suelen encontrar que en el conjunto de datos de entrenamiento una de las clases es minoritaria, es decir, en ella hay muy pocas muestras. Esto puede llegar a afectar a los algoritmos en su proceso de generalización y, en consecuencia, no poder diferenciar una clase de la otra."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    2316\n",
       "1    1147\n",
       "Name: toxicity, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "sample_data[\"toxicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    2317\n",
       "1     808\n",
       "2     269\n",
       "3      69\n",
       "Name: toxicity_level, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "sample_data[\"toxicity_level\"].value_counts()"
   ]
  },
  {
   "source": [
    "Tal y como se puede observar, en la variable `toxicity`, existen 2316 instancias de la clase 0. Por otra parte, la clase 1 contiene 1147. En este caso, aunque la clase 0 tenga el doble de observaciones, consideramos que no existe un problema de desbalanceo de clases ya que existen varias instancias de cada una de las dos clases. Sin embargo, en la variable `toxicity_level` sí que existe un desbalanceo de clases debido a que en las clases 2 y 3 existen pocas observaciones respecto a las otras dos. \n",
    "Por lo tanto, se ha decidido, una vez se evaluen los modelos correspondientes, balancear dicha variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Dividir el corpus en conjunto de entrenamiento y test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Hay que tener claro que, en primer lugar, se van a realizar los modelos de extracción de características y de Machine Learning para la variable `toxicity`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 111,
=======
   "execution_count": 9,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 112,
=======
   "execution_count": 10,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(sample_data['comment'], sample_data['toxicity'], test_size=0.3)"
   ]
  },
  {
   "source": [
    "### Extracción de características"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Bolsa de palabras (Bag-of-Words)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 113,
=======
   "execution_count": 11,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "ngram_vectorizer.fit(sample_data['comment'])\n",
    "train_X_count_ = ngram_vectorizer.transform(train_X)\n",
    "test_X_count_ = ngram_vectorizer.transform(test_X)\n"
   ]
  },
  {
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Una vez se ha dividido el corpus en train y test, se pasa a realizar un TF-IDF para luego entrenar los datos con una serie de modelos que se comentarán posteriormente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 114,
=======
   "execution_count": 12,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(sample_data['comment'])\n",
    "train_X_Tfidf = tfidf_vect.transform(train_X)\n",
    "test_X_Tfidf = tfidf_vect.transform(test_X)"
   ]
  },
  {
   "source": [
    "Se muestra un ejemplo de cómo queda el diccionario. Únicamente se mostrarán 30 de las palabras que lo forman."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 115,
=======
   "execution_count": 13,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'pensó': 9003, 'zumo': 12654, 'restar': 10533, 'gusta': 5821, 'afeitado': 489, 'seco': 10902, 'gent': 5666, 'asi': 1169, 'maten': 7609, 'alta': 714, 'mar': 7506, 'ma': 7330, 'inmigrant': 6520, 'porfavor': 9398, 'loss': 7284, 'valient': 12180, 'mejor': 7677, 'cortan': 2986, 'cabeza': 1758, 'socialista': 11201, 'izquierdista': 6781, 'racista': 10066, 'costumbr': 3013, 'lastima': 7000, 'volvio': 12504, 'loco': 7264, 'suicido': 11422, 'paso': 8862, 'preparado': 9538, 'cortar': 2988}\n"
     ]
    }
   ],
   "source": [
    "first30pairs = {k: tfidf_vect.vocabulary_[k] for k in list(tfidf_vect.vocabulary_)[:30]}\n",
    "print(first30pairs)"
   ]
  },
  {
   "source": [
    "#### Word2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**ATENCIÓN** \n",
    "\n",
    "En este caso, para el word2vec se ha encontrado un corpus preentrenado de `Google News`. No obstante, debido a que pesa alrededor de 1.5 Gb, se ha decidido no entrenar modelos de Machine Learning con Word-Embeddings.\n",
    "\n",
    "Aún así, se adjunta a continuación el corpus del cual estamos hablando.\n",
    "\n",
    "https://github.com/mmihaltz/word2vec-GoogleNews-vectors \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Evaluación de modelos - Variable `toxicity`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Support Vector Machines (SVM) - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Una vez obtenidos los coeficientes de importancia de cada uno de los términos, se pasa a realizar el entrenamiento de un modelo, en este caso se va a empezar por el modelo de Máquinas de Soporte Vectorial"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 116,
=======
   "execution_count": 14,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_tfidf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svm_clf_tfidf.fit(train_X_Tfidf,train_Y)\n",
    "predictions_SVM_tfidf = svm_clf_tfidf.predict(test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 117,
=======
   "execution_count": 15,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.619816494054852"
      ]
     },
     "metadata": {},
     "execution_count": 117
=======
       "0.6334462025594573"
      ]
     },
     "metadata": {},
     "execution_count": 15
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_svm_tfidf = f1_score(test_Y, predictions_SVM_tfidf, average='macro')\n",
    "score_svm_tfidf"
   ]
  },
  {
   "source": [
    "Una vez entrenado el corpus con TF-IDF junto con el algoritmo de SVM, se puede observar que su F1-Score es de 0.62. Para una primera evaluación de dicho modelo, se puede decir que su resultado es mejorable pero no es un valor del todo malo ya que el valor óptimo de dicha métrica sería de 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Support Vector Machines (SVM) - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 118,
=======
   "execution_count": 51,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_cv = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svm_clf_cv.fit(train_X_count_,train_Y)\n",
    "predictions_SVM_cv = svm_clf_cv.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 119,
=======
   "execution_count": 52,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.6735740519492013"
      ]
     },
     "metadata": {},
     "execution_count": 119
=======
       "0.496582039814084"
      ]
     },
     "metadata": {},
     "execution_count": 52
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_svm_cv = f1_score(test_Y, predictions_SVM_cv, average='macro')\n",
    "score_svm_cv"
   ]
  },
  {
   "source": [
    "Sin embargo, utilizando un Count Vectorizer con Unigramas, el F1-Score es un poco superior al que se ha entrenado con TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Decision Tree - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 120,
=======
   "execution_count": 18,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_tfidf = DecisionTreeClassifier()\n",
    "tree_clf_tfidf.fit(train_X_Tfidf,train_Y)\n",
    "predictions_tree_tfidf = tree_clf_tfidf.predict(test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 121,
=======
   "execution_count": 19,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.6444601146842902"
      ]
     },
     "metadata": {},
     "execution_count": 121
=======
       "0.627743244409911"
      ]
     },
     "metadata": {},
     "execution_count": 19
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_tree_tfidf = f1_score(test_Y, predictions_tree_tfidf, average='macro')\n",
    "score_tree_tfidf"
   ]
  },
  {
   "source": [
    "En este caso se ha entrenado un árbol de decisión (también con la previa extracción de características de TF-IDF) y se puede observar como su F1-Score es muy similar al del modelo de Máquinas de Soporte Vectorial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Decision Tree - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 122,
=======
   "execution_count": 20,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_cv = DecisionTreeClassifier()\n",
    "tree_clf_cv.fit(train_X_count_,train_Y)\n",
    "predictions_tree_cv = tree_clf_cv.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 123,
=======
   "execution_count": 21,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.6325118857650485"
      ]
     },
     "metadata": {},
     "execution_count": 123
=======
       "0.6503487687603211"
      ]
     },
     "metadata": {},
     "execution_count": 21
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_tree_cv = f1_score(test_Y, predictions_tree_cv, average='macro')\n",
    "score_tree_cv"
   ]
  },
  {
   "source": [
    "Utilizando Count Vectorizer con Unigramas, el F1-Score es muy similar, por no decir idéntico, al entrenado con TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Logistic Regression - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 124,
=======
   "execution_count": 22,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_clf_tfidf = LogisticRegression()\n",
    "logreg_clf_tfidf.fit(train_X_Tfidf,train_Y)\n",
    "predictions_logreg_tfidf = logreg_clf_tfidf.predict(test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 125,
=======
   "execution_count": 23,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.5275608452977754"
      ]
     },
     "metadata": {},
     "execution_count": 125
=======
       "0.5450494539460128"
      ]
     },
     "metadata": {},
     "execution_count": 23
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_logreg_tfidf = f1_score(test_Y, predictions_logreg_tfidf, average='macro')\n",
    "score_logreg_tfidf"
   ]
  },
  {
   "source": [
    "En tercer caso, se ha evaluado un modelo de regresión logística y su valor de F1-Score es muy inferior al de los anteriores modelos realizados (0.57 aproximadamente)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Logistic Regression - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 126,
=======
   "execution_count": 24,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_clf_cv = LogisticRegression()\n",
    "logreg_clf_cv.fit(train_X_count_,train_Y)\n",
    "predictions_logreg_cv = logreg_clf_cv.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 127,
=======
   "execution_count": 25,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.6594486889266176"
      ]
     },
     "metadata": {},
     "execution_count": 127
=======
       "0.657547392854356"
      ]
     },
     "metadata": {},
     "execution_count": 25
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_logreg_cv = f1_score(test_Y, predictions_logreg_cv, average='macro')\n",
    "score_logreg_cv"
   ]
  },
  {
   "source": [
    "Como se puede observar, utilizando Count Vectorizer con unigramas junto con Regresión Logística, el F1-Score es muy superior que usando TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Perceptrón multicapa - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 128,
=======
   "execution_count": 26,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "mlp_clf.fit(train_X_Tfidf,train_Y)\n",
    "predictions_mlp = mlp_clf.predict(test_X_Tfidf)"
   ]
  },
  {
   "source": [
    "No hace falta ejecutarlo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 129,
=======
   "execution_count": 27,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.6308206157727363"
      ]
     },
     "metadata": {},
     "execution_count": 129
=======
       "0.6524137394483992"
      ]
     },
     "metadata": {},
     "execution_count": 27
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_mlp = f1_score(test_Y, predictions_mlp, average='macro')\n",
    "score_mlp"
   ]
  },
  {
   "source": [
    "Finalmente, se ha evaluado un perceptrón multicapa y se ha obtenido un F1-Score muy similar a los modelos de Regresión Logística y Máquinas de Soporte Vectorial"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Perceptrón multicapa - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 130,
=======
   "execution_count": 28,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf_cv = MLPClassifier(random_state=1, max_iter=300)\n",
    "mlp_clf_cv.fit(train_X_count_,train_Y)\n",
    "predictions_mlp_cv = mlp_clf_cv.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 131,
=======
   "execution_count": 29,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.6627344843271811"
      ]
     },
     "metadata": {},
     "execution_count": 131
=======
       "0.6462881616516583"
      ]
     },
     "metadata": {},
     "execution_count": 29
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_mlp_cv = f1_score(test_Y, predictions_mlp_cv, average='macro')\n",
    "score_mlp_cv"
   ]
  },
  {
   "source": [
    "En este caso, utilizando Count Vectorizer con Perceptrón multicapa, se puede observar que el F1-Score es un poco superior que utilizando TF-IDF (0.66)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Todo lo que se ha realizado anteriormente ha sido mediante la variable `toxicity`. Debido a que hay dos tareas, a continuación, se pasa a realizar el mismo proceso para la variable desglosada de `toxicity_level`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 132,
=======
   "execution_count": 30,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_, test_X_, train_Y_, test_Y_ = train_test_split(sample_data['comment'], sample_data['toxicity_level'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 133,
=======
   "execution_count": 31,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ = TfidfVectorizer()\n",
    "tfidf_vect_.fit(sample_data['comment'])\n",
    "train_X_Tfidf_ = tfidf_vect_.transform(train_X_)\n",
    "test_X_Tfidf_ = tfidf_vect_.transform(test_X_)"
   ]
  },
  {
   "source": [
    "### Evaluación de modelos `TF-IDF` - Variable `toxicity_level`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Support Vector Machines (SVM) - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 134,
=======
   "execution_count": 32,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_ = SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight=\"balanced\")\n",
    "svm_clf_.fit(train_X_Tfidf_,train_Y_)\n",
    "predictions_SVM_ = svm_clf_.predict(test_X_Tfidf_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 135,
=======
   "execution_count": 33,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.27948129806329336"
      ]
     },
     "metadata": {},
     "execution_count": 135
=======
       "0.31494777010003977"
      ]
     },
     "metadata": {},
     "execution_count": 33
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_svm_ = f1_score(test_Y_, predictions_SVM_, average='macro')\n",
    "score_svm_"
   ]
  },
  {
   "source": [
    "En este caso, utilizando la variable `toxicity_level`, se puede observar como el F1-Score ha disminuido radicalmente respecto a la anterior variable (0.40 aproximadamente)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Decision Tree - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 136,
=======
   "execution_count": 34,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_ = DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "tree_clf_.fit(train_X_Tfidf_,train_Y_)\n",
    "predictions_tree_ = tree_clf_.predict(test_X_Tfidf_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 137,
=======
   "execution_count": 35,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.36577675962063516"
      ]
     },
     "metadata": {},
     "execution_count": 137
=======
       "0.3651677596740282"
      ]
     },
     "metadata": {},
     "execution_count": 35
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_tree_ = f1_score(test_Y_, predictions_tree_, average='macro')\n",
    "score_tree_"
   ]
  },
  {
   "source": [
    "El F1-Score utilizando un árbol de decisión es de 0.31, un resultado bastante pobre."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Logistic Regression - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 138,
=======
   "execution_count": 36,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_clf_ = LogisticRegression(class_weight=\"balanced\")\n",
    "logreg_clf_.fit(train_X_Tfidf_,train_Y_)\n",
    "predictions_logreg_ = logreg_clf_.predict(test_X_Tfidf_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 139,
=======
   "execution_count": 37,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.3414378379588261"
      ]
     },
     "metadata": {},
     "execution_count": 139
=======
       "0.312941742941804"
      ]
     },
     "metadata": {},
     "execution_count": 37
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_logreg_ = f1_score(test_Y_, predictions_logreg_, average='macro')\n",
    "score_logreg_"
   ]
  },
  {
   "source": [
    "La regresión logística mejora un poco respecto del árbol de decisión, pero aún así el score sigue siendo bastante bajo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Perceptrón multicapa - TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 140,
=======
   "execution_count": 38,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf_ = MLPClassifier(random_state=1, max_iter=300)\n",
    "mlp_clf_.fit(train_X_Tfidf_,train_Y_)\n",
    "predictions_mlp_ = mlp_clf_.predict(test_X_Tfidf_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 141,
=======
   "execution_count": 39,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.32376110080094894"
      ]
     },
     "metadata": {},
     "execution_count": 141
=======
       "0.2711521598480594"
      ]
     },
     "metadata": {},
     "execution_count": 39
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_mlp_ = f1_score(test_Y_, predictions_mlp_, average='macro')\n",
    "score_mlp_"
   ]
  },
  {
   "source": [
    "A pesar de que el perceptrón multicapa no recibe como parámetro `class_weight=\"balanced\"`, se decidió entrenar dicho modelo porque se pensó que podría mejorar los modelos anteriores. Resulta que su score da incluso peor que el resto."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
<<<<<<< HEAD
    "#### Support Vector Machines (SVM) - Count Vectorizer con Unigramas"
=======
    "### Evaluación de modelos `CountVectorizer` - Variable `toxicity_level`"
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 159,
=======
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_, test_X_, train_Y_, test_Y_ = train_test_split(sample_data['comment'], sample_data['toxicity_level'], test_size=0.3)"
   ]
  },
  {
   "source": [
    "#### Bolsa de palabras (Bag-of-Words)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "ngram_vectorizer.fit(sample_data['comment'])\n",
    "train_X_count_ = ngram_vectorizer.transform(train_X_)\n",
<<<<<<< HEAD
    "test_X_count_ = ngram_vectorizer.transform(test_X_)\n",
    "\n",
    "svm_clf_cv_=SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight=\"balanced\")\n",
=======
    "test_X_count_ = ngram_vectorizer.transform(test_X_)"
   ]
  },
  {
   "source": [
    "#### Support Vector Machines (SVM) - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_cv_=SVC(C=1.0,kernel='linear',degree=3,gamma='auto',class_weight=\"balanced\")\n",
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    "svm_clf_cv_.fit(train_X_count_,train_Y_)\n",
    "predictions_SVM_cv_=svm_clf_cv_.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 165,
=======
   "execution_count": 65,
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.3435467031450613"
      ]
     },
     "metadata": {},
     "execution_count": 165
=======
       "0.3912258345961336"
      ]
     },
     "metadata": {},
     "execution_count": 65
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    }
   ],
   "source": [
    "score_svm_cv_=f1_score(test_Y_,predictions_SVM_cv_,average='macro')\n",
    "score_svm_cv_"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuputamadre=DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "tuputamadre.fit(train_X_count_,train_Y_)\n",
    "predictions_tree_cv_=tuputamadre.predict(test_X_count_)\n"
=======
   "source": [
    "#### Decision Tree - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_cv_=DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "tree_clf_cv_.fit(train_X_count_,train_Y_)\n",
    "predictions_tree_cv_=tree_clf_cv_.predict(test_X_count_)"
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-163-b0e30a1d443a>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-163-b0e30a1d443a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    score_tree_cv_=f1_score(test_Y_, predictions_tree_cv_, average='macro')\u001b[0m\n\u001b[1;37m                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "score_tree_cv_=f1_score(test_Y_, predictions_tree_cv_, average='macro')\n",
=======
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.35440324878633145"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "score_tree_cv_=f1_score(test_Y_,predictions_tree_cv_,average='macro')\n",
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
    "score_tree_cv_"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-166-5e29590745fd>, line 3)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-166-5e29590745fd>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    tree_clf_cv_ = DecisionTreeClassifier(class_weight=\"balanced\")\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tree_clf_cv_ = DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "tree_clf_cv_.fit(train_X_count_,train_Y_)\n",
    "predictions_tree_cv_ = tree_clf_cv_.predict(test_X_count_)\n",
    "\n",
    "score_tree_cv_ = f1_score(test_Y_, predictions_tree_cv_, average='macro')\n",
    "score_tree_cv_\n",
    "\n",
    "logreg_clf_cv_ = LogisticRegression(class_weight=\"balanced\")\n",
    "logreg_clf_cv_.fit(train_X_count_,train_Y_)\n",
    "predictions_logreg_cv_ = logreg_clf_cv_.predict(test_X_count_)\n",
    "\n",
    "score_logreg_cv_ = f1_score(test_Y_, predictions_logreg_cv_, average='macro')\n",
    "score_logreg_cv_\n",
    "\n",
    "mlp_clf_cv_ = MLPClassifier(random_state=1, max_iter=300)\n",
    "mlp_clf_cv_.fit(train_X_count_,train_Y_)\n",
    "predictions_mlp_cv_ = mlp_clf_cv_.predict(test_X_count_)\n",
    "\n",
    "score_mlp_cv_ = f1_score(test_Y_, predictions_mlp_cv_, average='macro')\n",
    "score_mlp_cv_"
   ]
=======
   "source": [
    "#### Logistic Regression - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_clf_cv_=LogisticRegression(class_weight=\"balanced\")\n",
    "logreg_clf_cv_.fit(train_X_count_,train_Y_)\n",
    "predictions_logreg_cv_=logreg_clf_cv_.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4108374384236453"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "score_logreg_cv_=f1_score(test_Y_,predictions_logreg_cv_,average='macro')\n",
    "score_logreg_cv_"
   ]
  },
  {
   "source": [
    "#### Perceptrón multicapa - Count Vectorizer con Unigramas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf_cv_=MLPClassifier(random_state=1,max_iter=300)\n",
    "mlp_clf_cv_.fit(train_X_count_,train_Y_)\n",
    "predictions_mlp_cv_=mlp_clf_cv_.predict(test_X_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3148230644332699"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "score_mlp_cv_=f1_score(test_Y_,predictions_mlp_cv_,average='macro')\n",
    "score_mlp_cv_"
   ]
  },
  {
   "source": [
    "El perceptrón multicapa se posiciona como el mejor modelo para `toxicity_level`, pero su f1-score deja mucho que desear."
   ],
   "cell_type": "markdown",
   "metadata": {}
>>>>>>> 12dc02c7c4bbadd24ab2de4ecb6524042f3f05a0
  }
 ]
}